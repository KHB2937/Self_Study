{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3390b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa2f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3786dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\a1154\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddcea9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Although', 'it', \"'s\", 'not', 'a', 'happily-ever-after', 'ending', ',', 'it', 'is', 'very', 'realistic', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Although it's not a happily-ever-after ending, it is very realistic.\"\n",
    "\n",
    "# 단어 토큰화\n",
    "tokenized_words = word_tokenize(text)\n",
    "\n",
    "print(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e6db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 토큰화 후 정제 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91e00ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text import TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96037d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = TEXT\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da8c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60951ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 단어 토큰 리스트\n",
    "tokenized_words = word_tokenize(corpus)\n",
    "\n",
    "# 파이썬의 Counter 모듈을 통해 단어의 빈도수 카운트하여 단어 집합 생성\n",
    "vocab = Counter(tokenized_words)\n",
    "\n",
    "# 빈도수가 2 이하인 단어 리스트 추출\n",
    "uncommon_words = [key for key, value in vocab.items() if value <= 2]\n",
    "\n",
    "# 빈도수가 2 이하인 단어들만 제거한 결과를 따로 저장\n",
    "cleaned_by_freq = [word for word in tokenized_words if word not in uncommon_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec6bfdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 30, '.': 28, ',': 21, 'of': 15, 'and': 14, 'to': 13, 'a': 12, 'military': 12, 'in': 12, 'people': 9, 'on': 9, 'are': 9, 'for': 7, 'this': 7, 'that': 6, 'I': 5, 'The': 5, 'you': 5, 'not': 4, 'or': 4, 'about': 4, 'US': 4, 'at': 4, 'every': 4, 'it': 4, 'make': 4, 'was': 4, 'movie': 3, 'be': 3, 'who': 3, 'they': 3, 'Abu-Gharib': 3, 'makes': 3, 'number': 3, 'million': 3, 'with': 3, 'total': 3, 'would': 3, 'an': 3, 'there': 3, 'days': 3, 'hour': 3, 'minimum': 3, 'get': 3, 'comments': 2, ')': 2, 'know': 2, 'nothing': 2, 'base': 2, 'state': 2, 'world': 2, 'time': 2, ':': 2, '2.3': 2, 'indicted': 2, 'than': 2, 'That': 2, \"'s\": 2, 'but': 2, 'reality': 2, 'is': 2, 'first': 2, 'aid': 2, 'When': 2, 'their': 2, 'Within': 2, 'hours': 2, 'food': 2, 'months': 2, 'But': 2, 'website': 2, 'men': 2, 'women': 2, 'so': 2, 'personal': 2, 'gain': 2, 'under': 2, '40': 2, 'work': 2, 'week': 2, 'much': 2, 'ranks': 2, 'degrees': 2, 'After': 1, 'reading': 1, 'am': 1, 'sure': 1, 'whether': 1, 'should': 1, 'angry': 1, 'sad': 1, 'sickened': 1, 'Seeing': 1, 'typical': 1, 'absolutely': 1, 'b': 1, 'everything': 1, 'think': 1, 'movies': 1, 'like': 1, 'CNN': 1, 'reports': 1, 'me': 1, 'wonder': 1, 'intellectual': 1, 'stimulation': 1, 'At': 1, 'type': 1, '1.4': 1, 'Active': 1, 'Duty': 1, 'another': 1, 'almost': 1, '900,000': 1, 'Guard': 1, 'Reserves': 1, 'roughly': 1, 'abuses': 1, 'Currently': 1, 'less': 1, '20': 1, '.00083': 1, '%': 1, 'Even': 1, 'if': 1, 'indict': 1, 'single': 1, 'member': 1, 'ever': 1, 'stepped': 1, 'come': 1, 'close': 1, 'making': 1, 'whole': 1, 'flaws': 1, 'take': 1, 'YEARS': 1, 'cover': 1, 'understand': 1, 'supposed': 1, 'sarcastic': 1, 'writer': 1, 'director': 1, 'trying': 1, 'commentary': 1, 'without': 1, 'enemy': 1, 'fight': 1, 'In': 1, 'has': 1, 'been': 1, 'its': 1, 'busiest': 1, 'when': 1, 'conflicts': 1, 'going': 1, 'called': 1, 'disaster': 1, 'relief': 1, 'humanitarian': 1, 'missions': 1, 'tsunami': 1, 'hit': 1, 'Indonesia': 1, 'devestating': 1, 'region': 1, 'scene': 1, 'chaos': 1, 'situation': 1, 'overwhelmed': 1, 'local': 1, 'governments': 1, 'leadership': 1, 'looked': 1, 'same': 1, 'mocks': 1, 'said': 1, 'happen': 1, 'reaching': 1, 'isolated': 1, 'villages': 1, 'airfields': 1, 'were': 1, 'built': 1, 'cargo': 1, 'aircraft': 1, 'started': 1, 'landing': 1, 'distribution': 1, 'system': 1, 'up': 1, 'running': 1, 'Hours': 1, 'weeks': 1, 'Yes': 1, 'unscrupulous': 1, 'then': 1, 'walk': 1, 'life': 1, 'occupation': 1, 'see': 1, 'decide': 1, 'all': 1, 'criminal': 1, 'minds': 1, 'thoughts': 1, 'destruction': 1, 'mayhem': 1, 'absolute': 1, 'disservice': 1, 'things': 1, 'do': 1, 'day': 1, 'One': 1, 'person': 1, 'even': 1, 'went': 1, 'far': 1, 'as': 1, 'say': 1, 'members': 1, 'Wow': 1, '!': 1, 'Entry': 1, 'level': 1, 'personnel': 1, 'just': 1, '$': 1, '8.00': 1, 'assuming': 1, 'Of': 1, 'course': 1, 'many': 1, 'more': 1, 'those': 1, 'harm': 1, 'way': 1, 'typically': 1, 'put': 1, '16-18': 1, 'end': 1, 'pay': 1, 'well': 1, 'wage': 1, 'So': 1, 'beg': 1, 'please': 1, 'yourself': 1, 'familiar': 1, 'around': 1, 'Go': 1, 'nearby': 1, 'visitor': 1, 'pass': 1, 'meet': 1, 'some': 1, 'quick': 1, 'disparage': 1, 'You': 1, 'surprised': 1, 'no': 1, 'longer': 1, 'accepts': 1, 'lieu': 1, 'prison': 1, 'They': 1, 'require': 1, 'GED': 1, 'prefer': 1, 'high': 1, 'school': 1, 'diploma': 1, 'middle': 1, 'expected': 1, 'undergraduate': 1, 'upper': 1, 'encouraged': 1, 'advanced': 1})\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b517abad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈도수가 2 이하인 단어 수: 234\n"
     ]
    }
   ],
   "source": [
    "print('빈도수가 2 이하인 단어 수:', len(uncommon_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f860f429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈도수 3 이상인 토큰 수: 306\n"
     ]
    }
   ],
   "source": [
    "cleaned_by_freq = [word for word in tokenized_words if word not in uncommon_words]\n",
    "\n",
    "print('빈도수 3 이상인 토큰 수:', len(cleaned_by_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3627de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 길이가 2 이하인 단어 제거\n",
    "cleaned_by_freq_len = []\n",
    "\n",
    "for word in cleaned_by_freq:\n",
    "    if len(word) > 2:\n",
    "        cleaned_by_freq_len.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7566e518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정제 전: ['the', 'for', 'this', 'movie', ',', 'I', 'not', 'I', 'be', ',']\n",
      "정제 후: ['the', 'for', 'this', 'movie', 'not', 'people', 'who', 'about', 'the', 'military']\n"
     ]
    }
   ],
   "source": [
    "# 정제 결과 확인\n",
    "print('정제 전:', cleaned_by_freq[:10])\n",
    "print('정제 후:', cleaned_by_freq_len[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f1c6527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 등장 빈도 기준 정제 함수\n",
    "def clean_by_freq(tokenized_words, cut_off_count):\n",
    "    # 파이썬의 Counter 모듈을 통해 단어의 빈도수 카운트하여 단어 집합 생성\n",
    "    vocab = Counter(tokenized_words)\n",
    "    \n",
    "    # 빈도수가 cut_off_count 이하인 단어 set 추출\n",
    "    uncommon_words = {key for key, value in vocab.items() if value <= cut_off_count}\n",
    "    \n",
    "    # uncommon_words에 포함되지 않는 단어 리스트 생성\n",
    "    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]\n",
    "\n",
    "    return cleaned_words\n",
    "\n",
    "# 단어 길이 기준 정제 함수\n",
    "def clean_by_len(tokenized_words, cut_off_length):\n",
    "    # 길이가 cut_off_length 이하인 단어 제거\n",
    "    cleaned_by_freq_len = []\n",
    "    \n",
    "    for word in tokenized_words:\n",
    "        if len(word) > cut_off_length:\n",
    "            cleaned_by_freq_len.append(word)\n",
    "\n",
    "    return cleaned_by_freq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "603926f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'for',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'not',\n",
       " 'people',\n",
       " 'who',\n",
       " 'about',\n",
       " 'the',\n",
       " 'military',\n",
       " 'who',\n",
       " 'they',\n",
       " 'they',\n",
       " 'this',\n",
       " 'about',\n",
       " 'Abu-Gharib',\n",
       " 'makes',\n",
       " 'about',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'this',\n",
       " 'the',\n",
       " 'number',\n",
       " 'people',\n",
       " 'the',\n",
       " 'military',\n",
       " 'million',\n",
       " 'with',\n",
       " 'the',\n",
       " 'and',\n",
       " 'for',\n",
       " 'total',\n",
       " 'million',\n",
       " 'The',\n",
       " 'number',\n",
       " 'people',\n",
       " 'for',\n",
       " 'Abu-Gharib',\n",
       " 'makes',\n",
       " 'the',\n",
       " 'total',\n",
       " 'people',\n",
       " 'the',\n",
       " 'total',\n",
       " 'military',\n",
       " 'you',\n",
       " 'every',\n",
       " 'military',\n",
       " 'that',\n",
       " 'Abu-Gharib',\n",
       " 'you',\n",
       " 'would',\n",
       " 'not',\n",
       " 'that',\n",
       " 'number',\n",
       " 'The',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'would',\n",
       " 'that',\n",
       " 'the',\n",
       " 'and',\n",
       " 'are',\n",
       " 'make',\n",
       " 'about',\n",
       " 'the',\n",
       " 'the',\n",
       " 'military',\n",
       " 'the',\n",
       " 'military',\n",
       " 'there',\n",
       " 'are',\n",
       " 'not',\n",
       " 'The',\n",
       " 'military',\n",
       " 'the',\n",
       " 'for',\n",
       " 'and',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'military',\n",
       " 'was',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'was',\n",
       " 'military',\n",
       " 'who',\n",
       " 'people',\n",
       " 'the',\n",
       " 'people',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'and',\n",
       " 'make',\n",
       " 'was',\n",
       " 'days',\n",
       " 'and',\n",
       " 'was',\n",
       " 'and',\n",
       " 'and',\n",
       " 'days',\n",
       " 'not',\n",
       " 'and',\n",
       " 'there',\n",
       " 'are',\n",
       " 'people',\n",
       " 'the',\n",
       " 'military',\n",
       " 'there',\n",
       " 'are',\n",
       " 'every',\n",
       " 'every',\n",
       " 'people',\n",
       " 'this',\n",
       " 'that',\n",
       " 'million',\n",
       " 'and',\n",
       " 'are',\n",
       " 'with',\n",
       " 'the',\n",
       " 'that',\n",
       " 'they',\n",
       " 'every',\n",
       " 'this',\n",
       " 'that',\n",
       " 'military',\n",
       " 'are',\n",
       " 'for',\n",
       " 'make',\n",
       " 'hour',\n",
       " 'hour',\n",
       " 'and',\n",
       " 'hour',\n",
       " 'days',\n",
       " 'for',\n",
       " 'makes',\n",
       " 'the',\n",
       " 'minimum',\n",
       " 'for',\n",
       " 'you',\n",
       " 'make',\n",
       " 'with',\n",
       " 'the',\n",
       " 'you',\n",
       " 'get',\n",
       " 'and',\n",
       " 'the',\n",
       " 'and',\n",
       " 'you',\n",
       " 'are',\n",
       " 'would',\n",
       " 'The',\n",
       " 'military',\n",
       " 'people',\n",
       " 'minimum',\n",
       " 'and',\n",
       " 'The',\n",
       " 'are',\n",
       " 'get',\n",
       " 'minimum',\n",
       " 'and',\n",
       " 'the',\n",
       " 'are',\n",
       " 'get']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문제의 조건에 맞게 함수를 호출해 주세요\n",
    "clean_by_freq = clean_by_freq(tokenized_words, 2)\n",
    "cleaned_words = clean_by_len(clean_by_freq, 2)\n",
    "\n",
    "cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce39d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b02834db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\a1154\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1a4a7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "{'yours', 'there', 've', 'from', 'can', \"that'll\", \"wasn't\", 'had', 'these', 'myself', 'your', \"don't\", 'aren', 'be', 'then', 'them', 'm', 'before', 'which', \"wouldn't\", 'weren', \"you're\", 'below', 'only', 'do', 'yourself', 'off', 'should', 'needn', 'd', 'until', 'shan', 'to', 'both', 'again', 'hasn', 'their', \"doesn't\", 'him', 'few', 'while', 'all', \"shan't\", 'because', 'wasn', 'up', 't', 'does', 'with', 'during', 'being', 'such', 'on', 'didn', 'isn', \"hasn't\", 's', 'very', 'under', \"didn't\", \"you'd\", 'o', 'themselves', \"mightn't\", 'doesn', 'himself', 'other', 'we', 'in', 'i', 'those', 'been', 'why', 'a', \"hadn't\", 'theirs', 'more', \"should've\", \"couldn't\", 'if', 'he', 'against', 'that', 'the', 'our', 'own', 'you', 'just', \"isn't\", 'hers', 'here', 'ma', 'yourselves', 'ain', 'once', 'are', 'an', 'll', 'wouldn', 'some', 'now', 'when', 'have', 'as', 'and', 'shouldn', 'down', 'couldn', 'over', 'than', 'has', 'of', 'no', 'haven', 'it', 'nor', 'what', 'his', \"she's\", \"weren't\", 'each', 'were', 'how', 'having', 'its', 'herself', 'itself', 'won', \"mustn't\", 'out', 'any', 'did', 'about', 'ours', 'by', \"shouldn't\", 'into', 'above', 'same', \"won't\", 'this', 'most', \"needn't\", 'me', 'hadn', 'will', 'they', 'mightn', \"you've\", 'too', 're', 'y', 'my', 'at', 'so', 'am', 'after', 'was', 'but', 'further', 'whom', 'don', 'is', 'not', 'her', 'through', 'doing', \"haven't\", \"it's\", 'for', 'where', 'mustn', 'who', \"aren't\", \"you'll\", 'she', 'between', 'ourselves', 'or'}\n"
     ]
    }
   ],
   "source": [
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "print('불용어 개수 :', len(stopwords_set))\n",
    "print(stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b07eaff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 178\n",
      "불용어 출력 : {'yours', 'there', 've', 'from', 'can', \"that'll\", \"wasn't\", 'had', 'these', 'myself', 'your', \"don't\", 'aren', 'be', 'then', 'them', 'm', 'before', 'which', \"wouldn't\", 'weren', \"you're\", 'below', 'only', 'do', 'yourself', 'off', 'should', 'needn', 'hello', 'd', 'until', 'shan', 'to', 'both', 'again', 'hasn', 'their', \"doesn't\", 'him', 'few', 'while', 'all', \"shan't\", 'because', 'wasn', 'up', 't', 'does', 'with', 'during', 'being', 'such', 'on', 'didn', 'isn', \"hasn't\", 's', 'very', 'under', \"didn't\", \"you'd\", 'o', 'themselves', \"mightn't\", 'doesn', 'himself', 'other', 'we', 'in', 'i', 'those', 'been', 'why', 'a', \"hadn't\", 'theirs', 'more', \"should've\", \"couldn't\", 'if', 'he', 'against', 'that', 'our', 'own', 'you', 'just', \"isn't\", 'hers', 'here', 'ma', 'yourselves', 'ain', 'once', 'are', 'an', 'll', 'wouldn', 'some', 'now', 'when', 'have', 'as', 'and', 'shouldn', 'down', 'couldn', 'over', 'than', 'has', 'of', 'no', 'haven', 'it', 'nor', 'what', 'his', \"she's\", \"weren't\", 'each', 'were', 'how', 'having', 'its', 'herself', 'itself', 'won', \"mustn't\", 'out', 'any', 'did', 'about', 'ours', 'by', \"shouldn't\", 'into', 'above', 'same', \"won't\", 'this', 'most', \"needn't\", 'hadn', 'will', 'they', 'mightn', \"you've\", 'too', 're', 'y', 'my', 'at', 'so', 'am', 'after', 'was', 'but', 'further', 'whom', 'don', 'is', 'not', 'her', 'through', 'doing', \"haven't\", \"it's\", 'for', 'where', 'mustn', 'who', \"aren't\", \"you'll\", 'she', 'between', 'ourselves', 'or'}\n"
     ]
    }
   ],
   "source": [
    "# 불용어 단어 추가 제거\n",
    "stopwords_set.add('hello')\n",
    "stopwords_set.remove('the')\n",
    "stopwords_set.remove('me')\n",
    "\n",
    "print('불용어 개수 :', len(stopwords_set))\n",
    "print('불용어 출력 :',stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db532f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me', 'ourselves', 'we', 'ours', 'our', 'i', 'my', 'myself'}\n"
     ]
    }
   ],
   "source": [
    "# NLTK가 기본 제공하는 불용어가 아니라 새로운 불용어 세트를 정의해서 사용할 수도 있습니다.\n",
    "my_stopwords_set = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves'}\n",
    "\n",
    "print(my_stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "948ff2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 등장 빈도와 단어 길이 기준으로 정제했던 cleaned_by_freq_len에서 불용어도 제거ㄱㄱ\n",
    "# cleaned_by_freq_len에 있는 토큰을 하나씩 순회하면서 단어가 불용어 세트에 있는지 확인\n",
    "# 하고, 없을 때에만 cleaned_words에 저장\n",
    "\n",
    "stop_words_set = set(stopwords.words('english'))\n",
    "\n",
    "# 불용어 제거\n",
    "cleaned_words = []\n",
    "\n",
    "for word in cleaned_by_freq_len:\n",
    "    if word not in stop_words_set:\n",
    "        cleaned_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ee49ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전: 169\n",
      "불용어 제거 후: 67\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거 결과 확인\n",
    "print('불용어 제거 전:', len(cleaned_by_freq_len))\n",
    "print('불용어 제거 후:', len(cleaned_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1764a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 함수\n",
    "def clean_by_stopwords(tokenized_words, stop_words_set):\n",
    "    cleaned_words = []\n",
    "    \n",
    "    for word in tokenized_words:\n",
    "        if word not in stop_words_set:\n",
    "            cleaned_words.append(word)\n",
    "            \n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f99e33a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['After',\n",
       " 'reading',\n",
       " 'the',\n",
       " 'comments',\n",
       " 'movie',\n",
       " ',',\n",
       " 'I',\n",
       " 'sure',\n",
       " 'whether',\n",
       " 'I',\n",
       " 'angry',\n",
       " ',',\n",
       " 'sad',\n",
       " 'sickened',\n",
       " '.',\n",
       " 'Seeing',\n",
       " 'comments',\n",
       " 'typical',\n",
       " 'people',\n",
       " ')',\n",
       " 'know',\n",
       " 'absolutely',\n",
       " 'nothing',\n",
       " 'the',\n",
       " 'military',\n",
       " 'b',\n",
       " ')',\n",
       " 'base',\n",
       " 'everything',\n",
       " 'think',\n",
       " 'know',\n",
       " 'movies',\n",
       " 'like',\n",
       " 'CNN',\n",
       " 'reports',\n",
       " 'Abu-Gharib',\n",
       " 'makes',\n",
       " 'me',\n",
       " 'wonder',\n",
       " 'the',\n",
       " 'state',\n",
       " 'intellectual',\n",
       " 'stimulation',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'At',\n",
       " 'the',\n",
       " 'time',\n",
       " 'I',\n",
       " 'type',\n",
       " 'the',\n",
       " 'number',\n",
       " 'people',\n",
       " 'the',\n",
       " 'US',\n",
       " 'military',\n",
       " ':',\n",
       " '1.4',\n",
       " 'million',\n",
       " 'Active',\n",
       " 'Duty',\n",
       " 'another',\n",
       " 'almost',\n",
       " '900,000',\n",
       " 'the',\n",
       " 'Guard',\n",
       " 'Reserves',\n",
       " 'total',\n",
       " 'roughly',\n",
       " '2.3',\n",
       " 'million',\n",
       " '.',\n",
       " 'The',\n",
       " 'number',\n",
       " 'people',\n",
       " 'indicted',\n",
       " 'abuses',\n",
       " 'Abu-Gharib',\n",
       " ':',\n",
       " 'Currently',\n",
       " 'less',\n",
       " '20',\n",
       " 'That',\n",
       " 'makes',\n",
       " 'the',\n",
       " 'total',\n",
       " 'people',\n",
       " 'indicted',\n",
       " '.00083',\n",
       " '%',\n",
       " 'the',\n",
       " 'total',\n",
       " 'military',\n",
       " '.',\n",
       " 'Even',\n",
       " 'indict',\n",
       " 'every',\n",
       " 'single',\n",
       " 'military',\n",
       " 'member',\n",
       " 'ever',\n",
       " 'stepped',\n",
       " 'Abu-Gharib',\n",
       " ',',\n",
       " 'would',\n",
       " 'come',\n",
       " 'close',\n",
       " 'making',\n",
       " 'whole',\n",
       " 'number',\n",
       " '.',\n",
       " 'The',\n",
       " 'flaws',\n",
       " 'movie',\n",
       " 'would',\n",
       " 'take',\n",
       " 'YEARS',\n",
       " 'cover',\n",
       " '.',\n",
       " 'I',\n",
       " 'understand',\n",
       " \"'s\",\n",
       " 'supposed',\n",
       " 'sarcastic',\n",
       " ',',\n",
       " 'reality',\n",
       " ',',\n",
       " 'the',\n",
       " 'writer',\n",
       " 'director',\n",
       " 'trying',\n",
       " 'make',\n",
       " 'commentary',\n",
       " 'the',\n",
       " 'state',\n",
       " 'the',\n",
       " 'military',\n",
       " 'without',\n",
       " 'enemy',\n",
       " 'fight',\n",
       " '.',\n",
       " 'In',\n",
       " 'reality',\n",
       " ',',\n",
       " 'the',\n",
       " 'US',\n",
       " 'military',\n",
       " 'busiest',\n",
       " 'conflicts',\n",
       " 'going',\n",
       " '.',\n",
       " 'The',\n",
       " 'military',\n",
       " 'the',\n",
       " 'first',\n",
       " 'called',\n",
       " 'disaster',\n",
       " 'relief',\n",
       " 'humanitarian',\n",
       " 'aid',\n",
       " 'missions',\n",
       " '.',\n",
       " 'When',\n",
       " 'the',\n",
       " 'tsunami',\n",
       " 'hit',\n",
       " 'Indonesia',\n",
       " ',',\n",
       " 'devestating',\n",
       " 'the',\n",
       " 'region',\n",
       " ',',\n",
       " 'the',\n",
       " 'US',\n",
       " 'military',\n",
       " 'the',\n",
       " 'first',\n",
       " 'the',\n",
       " 'scene',\n",
       " '.',\n",
       " 'When',\n",
       " 'the',\n",
       " 'chaos',\n",
       " 'the',\n",
       " 'situation',\n",
       " 'overwhelmed',\n",
       " 'the',\n",
       " 'local',\n",
       " 'governments',\n",
       " ',',\n",
       " 'military',\n",
       " 'leadership',\n",
       " 'looked',\n",
       " 'people',\n",
       " ',',\n",
       " 'the',\n",
       " 'people',\n",
       " 'movie',\n",
       " 'mocks',\n",
       " ',',\n",
       " 'said',\n",
       " 'make',\n",
       " 'happen',\n",
       " '.',\n",
       " 'Within',\n",
       " 'hours',\n",
       " ',',\n",
       " 'food',\n",
       " 'aid',\n",
       " 'reaching',\n",
       " 'isolated',\n",
       " 'villages',\n",
       " '.',\n",
       " 'Within',\n",
       " 'days',\n",
       " ',',\n",
       " 'airfields',\n",
       " 'built',\n",
       " ',',\n",
       " 'cargo',\n",
       " 'aircraft',\n",
       " 'started',\n",
       " 'landing',\n",
       " 'food',\n",
       " 'distribution',\n",
       " 'system',\n",
       " 'running',\n",
       " '.',\n",
       " 'Hours',\n",
       " 'days',\n",
       " ',',\n",
       " 'weeks',\n",
       " 'months',\n",
       " '.',\n",
       " 'Yes',\n",
       " 'unscrupulous',\n",
       " 'people',\n",
       " 'the',\n",
       " 'US',\n",
       " 'military',\n",
       " '.',\n",
       " 'But',\n",
       " ',',\n",
       " 'every',\n",
       " 'walk',\n",
       " 'life',\n",
       " ',',\n",
       " 'every',\n",
       " 'occupation',\n",
       " '.',\n",
       " 'But',\n",
       " 'see',\n",
       " 'people',\n",
       " 'website',\n",
       " 'decide',\n",
       " '2.3',\n",
       " 'million',\n",
       " 'men',\n",
       " 'women',\n",
       " 'criminal',\n",
       " ',',\n",
       " 'nothing',\n",
       " 'minds',\n",
       " 'thoughts',\n",
       " 'destruction',\n",
       " 'mayhem',\n",
       " 'absolute',\n",
       " 'disservice',\n",
       " 'the',\n",
       " 'things',\n",
       " 'every',\n",
       " 'day',\n",
       " '.',\n",
       " 'One',\n",
       " 'person',\n",
       " 'website',\n",
       " 'even',\n",
       " 'went',\n",
       " 'far',\n",
       " 'say',\n",
       " 'military',\n",
       " 'members',\n",
       " 'personal',\n",
       " 'gain',\n",
       " '.',\n",
       " 'Wow',\n",
       " '!',\n",
       " 'Entry',\n",
       " 'level',\n",
       " 'personnel',\n",
       " 'make',\n",
       " '$',\n",
       " '8.00',\n",
       " 'hour',\n",
       " 'assuming',\n",
       " '40',\n",
       " 'hour',\n",
       " 'work',\n",
       " 'week',\n",
       " '.',\n",
       " 'Of',\n",
       " 'course',\n",
       " ',',\n",
       " 'many',\n",
       " 'work',\n",
       " 'much',\n",
       " '40',\n",
       " 'hours',\n",
       " 'week',\n",
       " 'harm',\n",
       " \"'s\",\n",
       " 'way',\n",
       " 'typically',\n",
       " 'put',\n",
       " '16-18',\n",
       " 'hour',\n",
       " 'days',\n",
       " 'months',\n",
       " 'end',\n",
       " '.',\n",
       " 'That',\n",
       " 'makes',\n",
       " 'the',\n",
       " 'pay',\n",
       " 'well',\n",
       " 'minimum',\n",
       " 'wage',\n",
       " '.',\n",
       " 'So',\n",
       " 'much',\n",
       " 'personal',\n",
       " 'gain',\n",
       " '.',\n",
       " 'I',\n",
       " 'beg',\n",
       " ',',\n",
       " 'please',\n",
       " 'make',\n",
       " 'familiar',\n",
       " 'the',\n",
       " 'world',\n",
       " 'around',\n",
       " '.',\n",
       " 'Go',\n",
       " 'nearby',\n",
       " 'base',\n",
       " ',',\n",
       " 'get',\n",
       " 'visitor',\n",
       " 'pass',\n",
       " 'meet',\n",
       " 'the',\n",
       " 'men',\n",
       " 'women',\n",
       " 'quick',\n",
       " 'disparage',\n",
       " '.',\n",
       " 'You',\n",
       " 'would',\n",
       " 'surprised',\n",
       " '.',\n",
       " 'The',\n",
       " 'military',\n",
       " 'longer',\n",
       " 'accepts',\n",
       " 'people',\n",
       " 'lieu',\n",
       " 'prison',\n",
       " 'time',\n",
       " '.',\n",
       " 'They',\n",
       " 'require',\n",
       " 'minimum',\n",
       " 'GED',\n",
       " 'prefer',\n",
       " 'high',\n",
       " 'school',\n",
       " 'diploma',\n",
       " '.',\n",
       " 'The',\n",
       " 'middle',\n",
       " 'ranks',\n",
       " 'expected',\n",
       " 'get',\n",
       " 'minimum',\n",
       " 'undergraduate',\n",
       " 'degrees',\n",
       " 'the',\n",
       " 'upper',\n",
       " 'ranks',\n",
       " 'encouraged',\n",
       " 'get',\n",
       " 'advanced',\n",
       " 'degrees',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 코드\n",
    "clean_by_stopwords(tokenized_words, stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "537c3719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화 \n",
    "# 정규화 방법: 대소문자 통합\n",
    "# 대부분의 프로그래밍 언어는 대소문자를 구분\n",
    "# 그래서 코퍼스를 대문자나 소문자 중 하나로 통일하면 정규화가 된다\n",
    "\n",
    "# 영어 문법 상 대문자는 특수한 상황에서만 사용되고, \n",
    "# 보통은 소문자가 많이 사용 따라서 대문자를 소문자로 바꾸는게 일반적\n",
    "# 해당 과정에는 파이썬의 문자열 내장함수인 lower()가 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0e40598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what can i do for you? do your homework now.\n"
     ]
    }
   ],
   "source": [
    "text = \"What can I do for you? Do your homework now.\"\n",
    "\n",
    "# 소문자로 변환\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "990427b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화 방법: 규칙 기반 정규화\n",
    "# USA, US, U.S.는 형태가 다르지만 의미는 같아용\n",
    "# 표준어는 아니지만 Umm과 Ummmm도 같은 의미이기 때문에 정규화 할 수 있죠\n",
    "# 이런 단어들은 규칙을 정의해서 하나의 표현으로 통합할 수 있습니다\n",
    "\n",
    "# 아래 코드는 US를 USA로, Ummmm을 Umm으로 통합하는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d967d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동의어 사전\n",
    "# 같은 의미를 가지는 단어들을 어떤 형태로 정규화를 할지 미리 정한다.\n",
    "# 기준이 정해 졌다면? 정규화 할 단어들을 짝지어 딕셔너리 형태의 동의어 사전을 만들어 준다\n",
    "synonym_dict = {'US':'USA', 'U.S':'USA', 'Ummm':'Umm', 'Ummmm':'Umm' }\n",
    "text = \"She became a US citizen. Ummmm, I think, maybe and or.\"\n",
    "# 분석에 활용할 텍스트와 정규화 된 단어들을 저장할 리스트를 만들어 준다.\n",
    "normalized_words = []\n",
    "\n",
    "# 단어 토큰화\n",
    "tokenized_words = nltk.word_tokenize(text)\n",
    "\n",
    "for word in tokenized_words:\n",
    "    # 동의어 사전에 있는 단어라면, value에 해당하는 값으로 변환\n",
    "    if word in synonym_dict.keys():\n",
    "        word = synonym_dict[word]\n",
    "\n",
    "    normalized_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "434900af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'became', 'a', 'USA', 'citizen', '.', 'Umm', ',', 'I', 'think', ',', 'maybe', 'and', 'or', '.']\n"
     ]
    }
   ],
   "source": [
    "# 결과 확인\n",
    "print(normalized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fd855a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01835b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b3f46ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['You', 'are', 'so', 'lovely', '.', 'I', 'am', 'loving', 'you', 'now', '.']\n",
      "포터 스테머의 어간 추출 후: ['you', 'are', 'so', 'love', '.', 'i', 'am', 'love', 'you', 'now', '.']\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "text = \"You are so lovely. I am loving you now.\"\n",
    "porter_stemmed_words = []\n",
    "\n",
    "# 단어 토큰화\n",
    "tokenized_words = nltk.word_tokenize(text)\n",
    "\n",
    "# 토큰화 된 단어를 순회하며 어간을 추출하여 결과를 porter_stemmed_words에 추가\n",
    "# 참고로, NLTK의 porter_stemmer.stem() \n",
    "# 함수는 단어가 포터 스테머 알고리즘의 기준에 포함되면 추출된 어간을 반환하고, \n",
    "# 그렇지 않은 경우에는 원래의 단어를 반환해 줍니다.\n",
    "\n",
    "# 포터 스테머의 어간 추출\n",
    "for word in tokenized_words:\n",
    "    stem = porter_stemmer.stem(word)\n",
    "    porter_stemmed_words.append(stem)\n",
    "\n",
    "print('어간 추출 전 :', tokenized_words)\n",
    "print('포터 스테머의 어간 추출 후:', porter_stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25f9a84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['You', 'are', 'so', 'lovely', '.', 'I', 'am', 'loving', 'you', 'now', '.']\n",
      "포터 스테머의 어간 추출 후: ['you', 'ar', 'so', 'lov', '.', 'i', 'am', 'lov', 'you', 'now', '.']\n"
     ]
    }
   ],
   "source": [
    "# 랭커스터 스테머 알고리즘 사용\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "text = \"You are so lovely. I am loving you now.\"\n",
    "lancaster_stemmed_words = []\n",
    "\n",
    "# 랭커스터 스테머의 어간 추출\n",
    "for word in tokenized_words:\n",
    "    stem = lancaster_stemmer.stem(word)\n",
    "    lancaster_stemmed_words.append(stem)\n",
    "    \n",
    "print('어간 추출 전 :', tokenized_words)\n",
    "print('포터 스테머의 어간 추출 후:', lancaster_stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8dc85dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'are', 'so', 'love', '.', 'i', 'am', 'love', 'you', 'now', '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# 포터 스테머 어간 추출 함수\n",
    "def stemming_by_porter(tokenized_words):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    porter_stemmed_words = []\n",
    "\n",
    "    for word in tokenized_words:\n",
    "        stem = porter_stemmer.stem(word)\n",
    "        porter_stemmed_words.append(stem)\n",
    "\n",
    "    return porter_stemmed_words\n",
    "\n",
    "stemming_by_porter(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3dc68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eca2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb1b65e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ca7625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
